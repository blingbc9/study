재현율 : 실재 일어난 일중 , 내가 맞춘 것 (= 민감도) recall
정확도 : 내가 예상한 것 중 맞춘 것 Presision
F1 score :  2* (Presision * Recall / Presision + Recall  )

특이도 : 발생하지 않은 일들 중, 아닐 거라 예상한 것 Specificity Specificity
-----------------------------------------------------------------------------------
p.390
ROCR Curve 
- 2진 분류(binary classfication)  성능평에가 많이 사용됨
- 그래프가 왼쪽에 가까울 수록 올바르게 예측한 비율이 높다
- 그래프가 왼쪽에 가까울 수록 올바르게 예측한 비율이 높다
-ROC 곡선 하단의 면적을 AUROC(Area Under ROC)라고 한다.
AUROC의 면적이 클 수록 (1에 가까울수록) 모형의 성능이 좋다고 평가한다.


가로축 FPR(1-특이도(Specificity))
- 1인 케이스에 대해 1로 예측한 것(내가 맞춘 것)
세로축 TPR()
- 0인 케이스에 대해 1로 잘못 예측한 것
-----------------------------------------------------------------------------------
P 393
* 이익도표
- 분류모형 성능평가를 위한 척도

- 관측치에 대한 예측확률을 내림차순으로 정렬
- 데이터를 10개의 구간으로 나눈 뒤 반응률(response)을 산출 
- 기본향상도(baselin lift) 에 비해 반응률이 몇배나 높은지 계산, 이것을 향상도(lift)라 함
- 상위 등급일수록 높은 반응률(response),  향상도(lift)가 빠른 속도록 감소한다.
- 등급별로 향상도가 급격하게 변동할수록 좋은 모형, 향상도가 들쭉날쭉하면 안좋은 모형
-----------------------------------------------------------------------------------
P 395
데이터 마이닝 방법론
	분류분석
		로지스틱회귀분석, 의사결정나무, 베이지안 분류, 인공신경망, SVM
		의사결정나무 : 성과를 한 눈에 볼 수 있음,
		
		
	클러스터링
	회귀분석

분류분석
- 데이터가 어떤 '그룹'에 속하는지 예측
- 클러스터링과 유사 BUT 분류분석은 각 그룹이 정의되어 있다.
- 지도학습

예측분석
- 시계열분석처럼 시간에 따른 값 두 개만을 이용
- 한 개의 설명변수

분류와 예측의 차이
분류 : 레코드(튜플)의 범주형 속성값을 맞힘
예측 : 레코드(튜플)의 연속형 속성값을 맞힘

분류기법
- 회귀분석, 로지스틱 회귀분석
- 의사결정나무
- 베이지안 분류
- 인공신경망
- 지지도벡터기계 support vector machine
- k 최근접 이웃
- 규칙기반의 분류, 사례기반추론

p 397
로지스틱 회귀분석
- 반응변수가 범주형인 경우
- 예측변수가 주어질 때, 반응변수가 각 범주에 속할 확률이 얼마인지 추정, 추정확률을 기준치에 따라 분류하는 목적

오즈비, 오즈
오즈 : 성공할 확률이 실패할 확률의 몇 배인지 나나태는 확률,

선형회귀분석, 로지스틱회귀분석의 차이

선형회귀분석 
- 종속변수 : 연속형 변수
- 계수 추정법 : 최소제곱법
- 모형검정 : F-검정, T-검정

로지스틱 회귀분석
- 종속변수 : (0,1)
- 계수 추정법 : 최대우도추정법
- 모형 검정 : 카이제곱 검정(x**-test)

p 400
 의사결정나무
-입력값에 대하여 출력값을 예측하는 모형

예측력과 해석력
- 고객의 유치방안을 예측하고자 하는 경우 예측력에 치중
- 신용평가 심사시, 부적격 사유를 설명해야하므로 해석력에 치중

활용
1) 세분화 :  비슷한 특성을 갖는 그룹으로 분할, 특성을 파악
2) 분류 :  예측변수에 근거해 목표변수 범주를 몇 개의 등급으로 분류
3) 예측 : 규칙을 찾아내고 이를 기반으로 예측
4) 차원축소, 변수선택 :  
5) 교호작용효과
	- 여러 개의 예측변수들을 결합하여 목표변수에 작용하는 규칙을 파악
	- 범주의 병합, 연속형 변수의 이산화

장점
- 한 변수와 상관성이 높은 불필요한 변수가 있어도 영향을 받지 않는다
- 설명변수나 목표변수에 수치형변수와 범주형변수 모두 사용 가능

단점
- 새로운 자료에 대한 과대적합 가능성
- 분류 경계선 부근의 자료값에 대해 오차가 큼
- 설명 변수 간의 중요도를 판단하기 어려움

분리규칙
- 최적 분할 결정은, 불순도 감소량을 가장 크게하는 분할

분리기준
	이산형 목표변수: 카이제콥 통계량 p값, 지니지수, 엔트로피 지수
	연속형 목표변수: 분산분석에서 F통계량, 분산의 감소량

분순도의 측도
- 카이제콥 통계량: ((실제도수 - 기대도수)**/기대도수)의 합
- 지니지수 :노드의 불순도를 나타내는 값
	 지니지수가 클수록, 이질적이다, 순수도가 낮다
- 엔트로피 지수 : 무질서 정도에 대한 측도, 값이 클수록 순수도가 낮다

의사결정나무 알고리즘
CART : 
	목표변수가 범주형일 경우, 지니지수/ 연속형일 경우 분산을 이용한 이진분리
C4.5, C5.0
- 엔트로피지수(무질서 정도)활용

CHAID(CHI-squared Automatic Interaction Detection)
- 범주형 변수
- 카이제콥 통계량 사용
-------------------------------------------------------------------------------
221017

p 411 
앙상블 분석
- 과대/과소적합의 문제를 해결하기 위해, 여러 개의 예측모형들을 만든 후 모형들을 조합하여 하나의 최종 예측 모형을 만듬
- 앙상블 기법 종류 :  배깅, 부스팅, 랜덤포레스팅/ 스태킹

배깅
- 주어진 자료에서 여러개의 붓스트랩 자료들을 생성, 각 붓스트랩 자료에서 예측모형을 만든 후, 이를 결합하여 예측모형을 만듬
(붓스트랩: 주어진 자료에서 동일한 크기의 표본을 랜덤 복원추출로 뽑은 자료)

	보팅 
	여러 개의 모형으로 산출된 결과를 다수결에 의해서 최종 결과를 선정

- 배깅에서는 가지치기를 하지 않는다, 최대로 성장한 의사결정나무들을 활용한다
- 배깅은 훈련자료를 모집단으로 판단, 평균예측모형을 구하여 분산을 줄이고 예측력을 향상

부스팅
- 예측력이 약한모형을 결합하여 강한 예측모형을 구현
- Adaboot n개의 분류기에 각각 가중치를 설정, 이를 결합하여 최종 분류기를 만든다(가중치의 합은 1)
- 배깅에 비해 예측오차가 향상

랜덤 포레스트(random forest)
- 의사결정나무가 분산이 크다는 점을 고려하여, 약한 학습기들을 생성 후, 이를 선형 결합하여 최종학습기를 만든다
- 수천 개의 변수를 통해, 변수제거가 없이 실행, 정확도 좋은 성과
-


p 417
인공신경망 분석

- 신경망은 가중치를 반복적으로 조정하며 학습
- 인공 신경망은 신경망의 가중치를 초기화하고, 훈련 데이터를 통해 가중치를 갱신, 
  신경망의 구조를 선택하고, 활용할 알고리즘을 결정한 후, 신경망을 훈련시킨다.

인공신경망의 특징
 계산
- 뉴런은 전이함수, > 활성화 함수를 사용한다(activation function)
- 활성화 함수를 이용하여 출력을 결정하며, 입력신호의 가중치 합을 계산하여 임계값과 비교
- 가중치 합이 임계값보다 작으면 뉴련의 출력은 -1, 크커나 같으면 +1 출력

3) 뉴런의 활성화 함수
- 시그모이드 함수 : 로지스틱 회귀분석과 유사 0~1의 값
- softmax 함수 : (표준화지수 함수) 출력값이 여러개로 주어지고 목표치가 다범주인 경우, 각 범주에 속할 사후확률을 제공하는 함수
- Relu함수 : 입력값이 0 이하는 0, 0 이상은 x값을 가지는 함수 ( 최근 딥러닝에서 많이 활용)

바. 신경망 모형 구축시 고려사항
입력변수
- 신경망 모형은  복잡성으로 인하여 자료선택에 매우 민감
	범주형 변수 : 모든 범주에서 일정 빈도 이상의 값을 갖고, 각 범주의 빈도가 일정할 때
	연속형 변수 : 입력한 변수들의 값의 범위가 변수들간 큰 차이가 없을 때

- 연속형 변수의 경우 그 분포가 평균을 중심으로 대칭이 아니면 좋지 않음, 이에 아래와 같은 방법을 활용
	변환 : 고객의 소득 ( 대부분 평균 미만이고 특정 고객의 소득이 매우 큼) > 로그변환
	범주화 :  각 범주의 빈도가 비슷하게 되도록 설정
	 > 

가중치의 초기값과 다중 최소값 문제
- 역전파 알고리즘은 초기값에 따라 결과가 많이 달라짐, 초기값 선택이 중요
- 가중치가 0 이면 시그모이드 함수는 선형이 되고 신경망 모형은 근사적으로 선형모형이 된다.
- 일반적으로 초기값은 0 근처로 랜덤하게 선택하므로, 초기 모형은 선형모형에 가깝고, 가중치 값이 증가할 수록 비선형


학습모드
- 온라인 학습 모드
- 확률적 학습 모드
- 배치 학습 모드

은닉층(hidden layer) 은닉노드(hidden layer)
- 은닉층과 은닉노드가 많으면 가중치가 많아져 과대 적합문제발생
-  은닉층과 은닉노드가 적으면 과소적합 문제
- 가능하면 은닉층은 하나로 선정
- 은닉노드의 수는 적절히 큰 값으로 놓고 가중치를 감소 시키며 적용

과대 적합 문제
- 조기종료, 가중치 감소 기법
- 모형 적합 과정에서 검증오차 증가시, 반복을 중지하고 조기 종료 시행
- 성형모형의 능형회귀와 유사한 가중치 감소라는 벌점화 기법사용

221018 Night

군집분석
- 군집에 속한 객체들의 유사성과 서로 다른 군집에 속한 상이성을 규명
- 군집의 구조나 개수에 대한 가정없이 데이터들 사이의 거리를 기준으로 군집화를 유도함
- 상품구매행동, life style에 따른 소비자군 분류



나. 특징
1) 요인분석과의 차이점
	- 요인분석은 유사한 변수를 함께 묶어주는 것이 목적 ???
2) 판별분석과의 차이점
	- 판별분석은 사전에 집단이 나누어져 있는 자료를 기반, 새로운 데이터를 기존의 집단에 할당

2. 거리

연속형 변수
- 유클리디안 거리 : 데이터간의 유사성을 측정할 때 많이 사용(산포 정도x)

- 표준화 거리 : 표준편차로 척도 변환 후 > 유클리드안거리로 계산
		표준화시, 척도차이, 분산차이로 인한 왜곡을 피할 수 있다.
- 마할라노비스 거리: 사전 지식 없이는 표본공부산S를 계산할 수 없다
- 체비셰프 거리
- 멘하튼 거리 : 유클리디안 거리와 함께 가장 많이 사용
- 캔버라 거리 : 
- 민코우스키 거리 : 맨하탄 거리 + 유클리디안 거리
		L1거리(맨하탄거리), L2(유클리디안 거리)

범주형 변수
- 자카드 거리 : (합집합 - 교집합) / 합집합
- 자카드 계수 : 교집합 / 합집합
- 코사인 거리 : 유사도 기준으로 분류
- 코사인 유사도



3. 계층적 군집분석
- n개의 군집으로 시작, 군집의 개수를 줄여 나감
- 군집 형성 방법에는 합병형 방법, 분리형 방법

  최단연결법
	- n*n 거리행렬에서 가장 가까운 데이터를 묶어서 군집 형성
	- 최단거리로(min) 계산하여 거리행렬 수정
 
 최단연결법
	-거리 계산시 최장거리(max)를 거리로 계산하여 거리행렬을 수정

 평균연결법
	-거리 계산시 평균(mean)를 거리로 계산하여 거리행렬을 수정	

 와드연결법
	- 군집내 편차들의 제곱합을 고려한 방법
	- 군집간 정보 손실 최소화

 군집화


4. 비계층적 군집분석
- n개의 개체를 g개의 군집으로 나눌 수 있는 모든 가능한 방법을 점검해 최적화 군집 형성

K-means 군집분석
- 주어진 데이터를 k개의 클러스터로 죾는 알고리즘
- 거리차이의 분산을 최소화하는 방식

K-means 과정
- 군집의 개수와 초기값(seed)을 정함
- 가장 가까운 seed가 있는 군집으로 분류
- seed값을 다시 계산

K-means 특징
- 거리 계산이므로 연속형 변수에 활용 가능
- k개의 초기 중심값은 임으로 선택 가능, 가급적 멀리
- 초기 중심값의 선정에 따라 다른 결과
- 초기 중심으로 오차 제곱합을 최소화하는 탐욕적알고리즘( greedy) > 안정됨>but 최적이라는 보장 없음

장점 
- 단순, 빠름
- 계층적 군집분석에 비해 대량의 데이터 처리
- 사전 정보가 없어도 의미있는 자료 구조

단점
- 결과해석이 어려움
- 군집수, 가중치 정의가 어려움
- 잡음이나 이상값 영향 많음
- U형태의 군집일 경우, 성능 저하


5. 혼합 분포 군집
- 모형 기반(model-based), 모수적 모형(정규분포, 다변량 정규분포), 모수와 가중치를 추정
- 모수와 가중치의 추정에 EM알로리즘 사용

데이터 형태
- 분포형태가 다봉형의 형태를 띠므로, 단일 분포로 적절하지 않음, 대략 3개 정도의 정규분포 결합
- 이변량 정규분포의 결함, 
- 두 경우 모두 반드시 정규분포일 필요는 없다

EM알고리즘



혼합분포 군집모형의 특징
-  K-means 절차와 유사하지만, 확률분포를 도입하여 군집 수행
- 몇 개의 모수로 표현 가능, 서로 다른 크기나 모양의 군집을 찾을 수 있다
- 이상치 자료에 민감하므로 사전 조치 필요


SOM (Self Organizing Map)
- SOM은 비지도 신경망으로 고차원의 데이터를  이해하기 쉬운 저차원의 뉴런으로 정렬
- 이런 형상화는 입력 변수 위치 관계를 그대로 보존

구성
1) 입력층(입력벡터를 받는 층)
- 입력변수의 개수와 동일하게 뉴런 수가 존재
- 지도(map) : 입력층의 자료는 학습 후 경쟁층에 정렬(입력층, 경쟁층 완전 연결)

2) 경쟁층(2차원 격차로 구성된 층)
- 입력백터의 특성에 따라 벡터가 한 점으로 클러스터링
- som은 경쟁 학습으로 각각의 뉴런이  입력 벡터와 얼마나 가까운가를 계사하여, 연결 강도를 반복적으로 재조정하여 학습
- 입력패턴과 유사한 뉴런이 승자
- 승자 독식 구조, 승자와 유사한 연결 강도를 갖는 입력 패턴이 통일한 경쟁 뉴런으로 배열 

som의 특징
- 고차원의 데이터를 저차원의 지도형태로 형상화> 시각적 이해 쉬움
-패턴발견, 이미지 분석에 성능 좋음


신경망 모형 : SOM
신경망 : 오차역전파법, /입력층 , 은닉층, 출력층, /지도학습 
SOM : 경쟁학습법/ 입력층 ,경쟁층/ 비지도 학습














	