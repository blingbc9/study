재현율 : 실재 일어난 일중 , 내가 맞춘 것 (= 민감도) recall
정확도 : 내가 예상한 것 중 맞춘 것 Presision
F1 score :  2* (Presision * Recall / Presision + Recall  )

특이도 : 발생하지 않은 일들 중, 아닐 거라 예상한 것 Specificity Specificity
-----------------------------------------------------------------------------------
p.390
ROCR Curve 
- 2진 분류(binary classfication)  성능평에가 많이 사용됨
- 그래프가 왼쪽에 가까울 수록 올바르게 예측한 비율이 높다
- 그래프가 왼쪽에 가까울 수록 올바르게 예측한 비율이 높다
-ROC 곡선 하단의 면적을 AUROC(Area Under ROC)라고 한다.
AUROC의 면적이 클 수록 (1에 가까울수록) 모형의 성능이 좋다고 평가한다.


가로축 FPR(1-특이도(Specificity))
- 1인 케이스에 대해 1로 예측한 것(내가 맞춘 것)
세로축 TPR()
- 0인 케이스에 대해 1로 잘못 예측한 것
-----------------------------------------------------------------------------------
P 393
* 이익도표
- 분류모형 성능평가를 위한 척도

- 관측치에 대한 예측확률을 내림차순으로 정렬
- 데이터를 10개의 구간으로 나눈 뒤 반응률(response)을 산출 
- 기본향상도(baselin lift) 에 비해 반응률이 몇배나 높은지 계산, 이것을 향상도(lift)라 함
- 상위 등급일수록 높은 반응률(response),  향상도(lift)가 빠른 속도록 감소한다.
- 등급별로 향상도가 급격하게 변동할수록 좋은 모형, 향상도가 들쭉날쭉하면 안좋은 모형
-----------------------------------------------------------------------------------
P 395
데이터 마이닝 방법론
	분류분석
		로지스틱회귀분석, 의사결정나무, 베이지안 분류, 인공신경망, SVM
		의사결정나무 : 성과를 한 눈에 볼 수 있음,
		
		
	클러스터링
	회귀분석

분류분석
- 데이터가 어떤 '그룹'에 속하는지 예측
- 클러스터링과 유사 BUT 분류분석은 각 그룹이 정의되어 있다.
- 지도학습

예측분석
- 시계열분석처럼 시간에 따른 값 두 개만을 이용
- 한 개의 설명변수

분류와 예측의 차이
분류 : 레코드(튜플)의 범주형 속성값을 맞힘
예측 : 레코드(튜플)의 연속형 속성값을 맞힘

분류기법
- 회귀분석, 로지스틱 회귀분석
- 의사결정나무
- 베이지안 분류
- 인공신경망
- 지지도벡터기계 support vector machine
- k 최근접 이웃
- 규칙기반의 분류, 사례기반추론

p 397
로지스틱 회귀분석
- 반응변수가 범주형인 경우
- 예측변수가 주어질 때, 반응변수가 각 범주에 속할 확률이 얼마인지 추정, 추정확률을 기준치에 따라 분류하는 목적

오즈비, 오즈
오즈 : 성공할 확률이 실패할 확률의 몇 배인지 나나태는 확률,

선형회귀분석, 로지스틱회귀분석의 차이

선형회귀분석 
- 종속변수 : 연속형 변수
- 계수 추정법 : 최소제곱법
- 모형검정 : F-검정, T-검정

로지스틱 회귀분석
- 종속변수 : (0,1)
- 계수 추정법 : 최대우도추정법
- 모형 검정 : 카이제곱 검정(x**-test)

p 400
 의사결정나무
-입력값에 대하여 출력값을 예측하는 모형

예측력과 해석력
- 고객의 유치방안을 예측하고자 하는 경우 예측력에 치중
- 신용평가 심사시, 부적격 사유를 설명해야하므로 해석력에 치중

활용
1) 세분화 :  비슷한 특성을 갖는 그룹으로 분할, 특성을 파악
2) 분류 :  예측변수에 근거해 목표변수 범주를 몇 개의 등급으로 분류
3) 예측 : 규칙을 찾아내고 이를 기반으로 예측
4) 차원축소, 변수선택 :  
5) 교호작용효과
	- 여러 개의 예측변수들을 결합하여 목표변수에 작용하는 규칙을 파악
	- 범주의 병합, 연속형 변수의 이산화

장점
- 한 변수와 상관성이 높은 불필요한 변수가 있어도 영향을 받지 않는다
- 설명변수나 목표변수에 수치형변수와 범주형변수 모두 사용 가능

단점
- 새로운 자료에 대한 과대적합 가능성
- 분류 경계선 부근의 자료값에 대해 오차가 큼
- 설명 변수 간의 중요도를 판단하기 어려움

분리규칙
- 최적 분할 결정은, 불순도 감소량을 가장 크게하는 분할

분리기준
	이산형 목표변수: 카이제콥 통계량 p값, 지니지수, 엔트로피 지수
	연속형 목표변수: 분산분석에서 F통계량, 분산의 감소량

분순도의 측도
- 카이제콥 통계량: ((실제도수 - 기대도수)**/기대도수)의 합
- 지니지수 :노드의 불순도를 나타내는 값
	 지니지수가 클수록, 이질적이다, 순수도가 낮다
- 엔트로피 지수 : 무질서 정도에 대한 측도, 값이 클수록 순수도가 낮다

의사결정나무 알고리즘
CART : 
	목표변수가 범주형일 경우, 지니지수/ 연속형일 경우 분산을 이용한 이진분리
C4.5, C5.0
- 엔트로피지수(무질서 정도)활용

CHAID(CHI-squared Automatic Interaction Detection)
- 범주형 변수
- 카이제콥 통계량 사용
-------------------------------------------------------------------------------
221017

p 411 
앙상블 분석
- 과대/과소적합의 문제를 해결하기 위해, 여러 개의 예측모형들을 만든 후 모형들을 조합하여 하나의 최종 예측 모형을 만듬
- 앙상블 기법 종류 :  배깅, 부스팅, 랜덤포레스팅/ 스태킹

배깅
- 주어진 자료에서 여러개의 붓스트랩 자료들을 생성, 각 붓스트랩 자료에서 예측모형을 만든 후, 이를 결합하여 예측모형을 만듬
(붓스트랩: 주어진 자료에서 동일한 크기의 표본을 랜덤 복원추출로 뽑은 자료)

	보팅 
	여러 개의 모형으로 산출된 결과를 다수결에 의해서 최종 결과를 선정

- 배깅에서는 가지치기를 하지 않는다, 최대로 성장한 의사결정나무들을 활용한다
- 배깅은 훈련자료를 모집단으로 판단, 평균예측모형을 구하여 분산을 줄이고 예측력을 향상

부스팅
- 예측력이 약한모형을 결합하여 강한 예측모형을 구현
- Adaboot n개의 분류기에 각각 가중치를 설정, 이를 결합하여 최종 분류기를 만든다(가중치의 합은 1)
- 배깅에 비해 예측오차가 향상

랜덤 포레스트(random forest)
- 의사결정나무가 분산이 크다는 점을 고려하여, 약한 학습기들을 생성 후, 이를 선형 결합하여 최종학습기를 만든다
- 수천 개의 변수를 통해, 변수제거가 없이 실행, 정확도 좋은 성과
-


p 417
인공신경망 분석

- 신경망은 가중치를 반복적으로 조정하며 학습
- 인공 신경망은 신경망의 가중치를 초기화하고, 훈련 데이터를 통해 가중치를 갱신, 
  신경망의 구조를 선택하고, 활용할 알고리즘을 결정한 후, 신경망을 훈련시킨다.

인공신경망의 특징
 계산
- 뉴런은 전이함수, > 활성화 함수를 사용한다(activation function)
- 활성화 함수를 이용하여 출력을 결정하며, 입력신호의 가중치 합을 계산하여 임계값과 비교
- 가중치 합이 임계값보다 작으면 뉴련의 출력은 -1, 크커나 같으면 +1 출력

3) 뉴런의 활성화 함수
- 시그모이드 함수 : 로지스틱 회귀분석과 유사 0~1의 값
- softmax 함수 : (표준화지수 함수) 출력값이 여러개로 주어지고 목표치가 다범주인 경우, 각 범주에 속할 사후확률을 제공하는 함수
- Relu함수 : 입력값이 0 이하는 0, 0 이상은 x값을 가지는 함수 ( 최근 딥러닝에서 많이 활용)

바. 신경망 모형 구축시 고려사항
입력변수
- 신경망 모형은  복잡성으로 인하여 자료선택에 매우 민감
	범주형 변수 : 모든 범주에서 일정 빈도 이상의 값을 갖고, 각 범주의 빈도가 일정할 때
	연속형 변수 : 입력한 변수들의 값의 범위가 변수들간 큰 차이가 없을 때

- 연속형 변수의 경우 그 분포가 평균을 중심으로 대칭이 아니면 좋지 않음, 이에 아래와 같은 방법을 활용
	변환 : 고객의 소득 ( 대부분 평균 미만이고 특정 고객의 소득이 매우 큼) > 로그변환
	범주화 :  각 범주의 빈도가 비슷하게 되도록 설정
	 > 

가중치의 초기값과 다중 최소값 문제
- 역전파 알고리즘은 초기값에 따라 결과가 많이 달라짐, 초기값 선택이 중요
- 가중치가 0 이면 시그모이드 함수는 선형이 되고 신경망 모형은 근사적으로 선형모형이 된다.
- 일반적으로 초기값은 0 근처로 랜덤하게 선택하므로, 초기 모형은 선형모형에 가깝고, 가중치 값이 증가할 수록 비선형


학습모드
- 온라인 학습 모드
- 확률적 학습 모드
- 배치 학습 모드

은닉층(hidden layer) 은닉노드(hidden layer)
- 은닉층과 은닉노드가 많으면 가중치가 많아져 과대 적합문제발생
-  은닉층과 은닉노드가 적으면 과소적합 문제
- 가능하면 은닉층은 하나로 선정
- 은닉노드의 수는 적절히 큰 값으로 놓고 가중치를 감소 시키며 적용

과대 적합 문제
- 조기종료, 가중치 감소 기법
- 모형 적합 과정에서 검증오차 증가시, 반복을 중지하고 조기 종료 시행
- 성형모형의 능형회귀와 유사한 가중치 감소라는 벌점화 기버밧용



















	